12/14/22 10:38:16 ******************************************************
12/14/22 10:38:16 ** condor_scheduniv_exec.8202655.0 (CONDOR_DAGMAN) STARTING UP
12/14/22 10:38:16 ** /usr/bin/condor_dagman
12/14/22 10:38:16 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)
12/14/22 10:38:16 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON
12/14/22 10:38:16 ** $CondorVersion: 8.9.11 Dec 28 2020 BuildID: 526068 PackageID: 8.9.11-1 $
12/14/22 10:38:16 ** $CondorPlatform: x86_64_CentOS7 $
12/14/22 10:38:16 ** PID = 3614393
12/14/22 10:38:16 ** Log last touched time unavailable (No such file or directory)
12/14/22 10:38:16 ******************************************************
12/14/22 10:38:16 Using config source: /etc/condor/condor_config
12/14/22 10:38:16 Using local config sources: 
12/14/22 10:38:16    /etc/condor/config.d/00-batch_gahp_blahp.config
12/14/22 10:38:16    /etc/condor/config.d/10_security.config
12/14/22 10:38:16    /etc/condor/config.d/11_fairshares.config
12/14/22 10:38:16    /etc/condor/config.d/12_resourcelimits.config
12/14/22 10:38:16    /etc/condor/config.d/14_network.config
12/14/22 10:38:16    /etc/condor/config.d/21_schedd.config
12/14/22 10:38:16    /etc/condor/config.d/21_submit_requirements.config
12/14/22 10:38:16    /etc/condor/config.d/31_krbrenewal.config
12/14/22 10:38:16    /etc/condor/config.d/41_job_transform.config
12/14/22 10:38:16    /etc/condor/config.d/80_banning_rules.config
12/14/22 10:38:16    /etc/condor/config.d/99.config
12/14/22 10:38:16    /etc/condor/condor_config.local
12/14/22 10:38:16 config Macros = 256, Sorted = 256, StringBytes = 26099, TablesBytes = 9352
12/14/22 10:38:16 CLASSAD_CACHING is ENABLED
12/14/22 10:38:16 Daemon Log is logging: D_ALWAYS D_ERROR
12/14/22 10:38:16 DaemonCore: No command port requested.
12/14/22 10:38:16 DAGMAN_USE_STRICT setting: 1
12/14/22 10:38:16 DAGMAN_VERBOSITY setting: 3
12/14/22 10:38:16 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880
12/14/22 10:38:16 DAGMAN_DEBUG_CACHE_ENABLE setting: False
12/14/22 10:38:16 DAGMAN_SUBMIT_DELAY setting: 0
12/14/22 10:38:16 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6
12/14/22 10:38:16 DAGMAN_STARTUP_CYCLE_DETECT setting: False
12/14/22 10:38:16 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 100
12/14/22 10:38:16 DAGMAN_AGGRESSIVE_SUBMIT setting: False
12/14/22 10:38:16 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5
12/14/22 10:38:16 DAGMAN_QUEUE_UPDATE_INTERVAL setting: 300
12/14/22 10:38:16 DAGMAN_DEFAULT_PRIORITY setting: 0
12/14/22 10:38:16 DAGMAN_SUPPRESS_NOTIFICATION setting: True
12/14/22 10:38:16 allow_events (DAGMAN_ALLOW_EVENTS) setting: 114
12/14/22 10:38:16 DAGMAN_RETRY_SUBMIT_FIRST setting: True
12/14/22 10:38:16 DAGMAN_RETRY_NODE_FIRST setting: False
12/14/22 10:38:16 DAGMAN_MAX_JOBS_IDLE setting: 1000
12/14/22 10:38:16 DAGMAN_MAX_JOBS_SUBMITTED setting: 0
12/14/22 10:38:16 DAGMAN_MAX_PRE_SCRIPTS setting: 20
12/14/22 10:38:16 DAGMAN_MAX_POST_SCRIPTS setting: 20
12/14/22 10:38:16 DAGMAN_MAX_HOLD_SCRIPTS setting: 20
12/14/22 10:38:16 DAGMAN_MUNGE_NODE_NAMES setting: True
12/14/22 10:38:16 DAGMAN_PROHIBIT_MULTI_JOBS setting: False
12/14/22 10:38:16 DAGMAN_SUBMIT_DEPTH_FIRST setting: False
12/14/22 10:38:16 DAGMAN_ALWAYS_RUN_POST setting: False
12/14/22 10:38:16 DAGMAN_CONDOR_SUBMIT_EXE setting: /usr/bin/condor_submit
12/14/22 10:38:16 DAGMAN_USE_CONDOR_SUBMIT setting: True
12/14/22 10:38:16 DAGMAN_ABORT_DUPLICATES setting: True
12/14/22 10:38:16 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True
12/14/22 10:38:16 DAGMAN_PENDING_REPORT_INTERVAL setting: 600
12/14/22 10:38:16 DAGMAN_AUTO_RESCUE setting: True
12/14/22 10:38:16 DAGMAN_MAX_RESCUE_NUM setting: 100
12/14/22 10:38:16 DAGMAN_WRITE_PARTIAL_RESCUE setting: True
12/14/22 10:38:16 DAGMAN_DEFAULT_NODE_LOG setting: @(DAG_DIR)/@(DAG_FILE).nodes.log
12/14/22 10:38:16 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True
12/14/22 10:38:16 DAGMAN_MAX_JOB_HOLDS setting: 100
12/14/22 10:38:16 DAGMAN_HOLD_CLAIM_TIME setting: 20
12/14/22 10:38:16 ALL_DEBUG setting: 
12/14/22 10:38:16 DAGMAN_DEBUG setting: 
12/14/22 10:38:16 DAGMAN_SUPPRESS_JOB_LOGS setting: False
12/14/22 10:38:16 DAGMAN_REMOVE_NODE_JOBS setting: True
12/14/22 10:38:16 DAGMAN will adjust edges after parsing
12/14/22 10:38:16 argv[0] == "condor_scheduniv_exec.8202655.0"
12/14/22 10:38:16 argv[1] == "-Lockfile"
12/14/22 10:38:16 argv[2] == "/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.lock"
12/14/22 10:38:16 argv[3] == "-AutoRescue"
12/14/22 10:38:16 argv[4] == "1"
12/14/22 10:38:16 argv[5] == "-DoRescueFrom"
12/14/22 10:38:16 argv[6] == "0"
12/14/22 10:38:16 argv[7] == "-Dag"
12/14/22 10:38:16 argv[8] == "/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag"
12/14/22 10:38:16 argv[9] == "-Suppress_notification"
12/14/22 10:38:16 argv[10] == "-CsdVersion"
12/14/22 10:38:16 argv[11] == "$CondorVersion: 8.9.11 Dec 28 2020 BuildID: 526068 PackageID: 8.9.11-1 $"
12/14/22 10:38:16 argv[12] == "-Dagman"
12/14/22 10:38:16 argv[13] == "/usr/bin/condor_dagman"
12/14/22 10:38:16 Workflow batch-id: <8202655.0>
12/14/22 10:38:17 Workflow batch-name: <filler.dag+8202655>
12/14/22 10:38:17 Workflow accounting_group: <>
12/14/22 10:38:17 Workflow accounting_group_user: <>
12/14/22 10:38:17 Warning: failed to get attribute DAGNodeName
12/14/22 10:38:17 DAGMAN_LOG_ON_NFS_IS_ERROR setting: False
12/14/22 10:38:17 Default node log file is: </afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log>
12/14/22 10:38:17 DAG Lockfile will be written to /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.lock
12/14/22 10:38:17 DAG Input file is /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag
12/14/22 10:38:17 Parsing 1 dagfiles
12/14/22 10:38:17 Parsing /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag ...
12/14/22 10:38:17 Adjusting edges
12/14/22 10:38:17 Dag contains 21 total jobs
12/14/22 10:38:17 Bootstrapping...
12/14/22 10:38:17 Number of pre-completed nodes: 0
12/14/22 10:38:17 MultiLogFiles: truncating log file /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:17 DAG status: 0 (DAG_STATUS_OK)
12/14/22 10:38:17 Of 21 nodes total:
12/14/22 10:38:17  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
12/14/22 10:38:17   ===     ===      ===     ===     ===        ===      ===
12/14/22 10:38:17     0       0        0       0      20          1        0
12/14/22 10:38:17 0 job proc(s) currently held
12/14/22 10:38:17 DAGMan Runtime Statistics: [ EventCycleTimeCount = 0.0; EventCycleTimeSum = 0.0; LogProcessCycleTimeCount = 0.0; LogProcessCycleTimeSum = 0.0; SleepCycleTimeCount = 0.0; SleepCycleTimeSum = 0.0; SubmitCycleTimeCount = 0.0; SubmitCycleTimeSum = 0.0; ]
12/14/22 10:38:17 Registering condor_event_timer...
12/14/22 10:38:18 Submitting HTCondor Node 0 job(s)...
12/14/22 10:38:18 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:18 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:18 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:18 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '0 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '0 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=0 -a NAME' '=' '0 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:18 From submit: Submitting job(s).
12/14/22 10:38:18 From submit: 1 job(s) submitted to cluster 8202656.
12/14/22 10:38:18 From submit: WARNING: the line 'JOB = 0' was unused by condor_submit. Is it a typo?
12/14/22 10:38:18 	assigned HTCondor ID (8202656.0.0)
12/14/22 10:38:18 Submitting HTCondor Node 1 job(s)...
12/14/22 10:38:18 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:18 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:18 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:18 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '1 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '1 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=1 -a NAME' '=' '1 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:18 From submit: Submitting job(s).
12/14/22 10:38:18 From submit: 1 job(s) submitted to cluster 8202657.
12/14/22 10:38:18 From submit: WARNING: the line 'JOB = 1' was unused by condor_submit. Is it a typo?
12/14/22 10:38:18 	assigned HTCondor ID (8202657.0.0)
12/14/22 10:38:18 Submitting HTCondor Node 2 job(s)...
12/14/22 10:38:18 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:18 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:18 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:18 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '2 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '2 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=2 -a NAME' '=' '2 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:18 From submit: Submitting job(s).
12/14/22 10:38:18 From submit: 1 job(s) submitted to cluster 8202658.
12/14/22 10:38:18 From submit: WARNING: the line 'JOB = 2' was unused by condor_submit. Is it a typo?
12/14/22 10:38:18 	assigned HTCondor ID (8202658.0.0)
12/14/22 10:38:18 Submitting HTCondor Node 3 job(s)...
12/14/22 10:38:18 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:18 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:18 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:18 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '3 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '3 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=3 -a NAME' '=' '3 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:18 From submit: Submitting job(s).
12/14/22 10:38:18 From submit: 1 job(s) submitted to cluster 8202659.
12/14/22 10:38:18 From submit: WARNING: the line 'JOB = 3' was unused by condor_submit. Is it a typo?
12/14/22 10:38:18 	assigned HTCondor ID (8202659.0.0)
12/14/22 10:38:18 Submitting HTCondor Node 4 job(s)...
12/14/22 10:38:18 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:18 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:18 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:18 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '4 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '4 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=4 -a NAME' '=' '4 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:19 From submit: Submitting job(s).
12/14/22 10:38:19 From submit: 1 job(s) submitted to cluster 8202660.
12/14/22 10:38:19 From submit: WARNING: the line 'JOB = 4' was unused by condor_submit. Is it a typo?
12/14/22 10:38:19 	assigned HTCondor ID (8202660.0.0)
12/14/22 10:38:19 Submitting HTCondor Node 5 job(s)...
12/14/22 10:38:19 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:19 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:19 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:19 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '5 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '5 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=5 -a NAME' '=' '5 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:19 From submit: Submitting job(s).
12/14/22 10:38:19 From submit: 1 job(s) submitted to cluster 8202661.
12/14/22 10:38:19 From submit: WARNING: the line 'JOB = 5' was unused by condor_submit. Is it a typo?
12/14/22 10:38:19 	assigned HTCondor ID (8202661.0.0)
12/14/22 10:38:19 Submitting HTCondor Node 6 job(s)...
12/14/22 10:38:19 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:19 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:19 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:19 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '6 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '6 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=6 -a NAME' '=' '6 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:19 From submit: Submitting job(s).
12/14/22 10:38:19 From submit: 1 job(s) submitted to cluster 8202662.
12/14/22 10:38:19 From submit: WARNING: the line 'JOB = 6' was unused by condor_submit. Is it a typo?
12/14/22 10:38:19 	assigned HTCondor ID (8202662.0.0)
12/14/22 10:38:19 Submitting HTCondor Node 7 job(s)...
12/14/22 10:38:19 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:19 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:19 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:19 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '7 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '7 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=7 -a NAME' '=' '7 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:19 From submit: Submitting job(s).
12/14/22 10:38:19 From submit: 1 job(s) submitted to cluster 8202663.
12/14/22 10:38:19 From submit: WARNING: the line 'JOB = 7' was unused by condor_submit. Is it a typo?
12/14/22 10:38:19 	assigned HTCondor ID (8202663.0.0)
12/14/22 10:38:19 Submitting HTCondor Node 8 job(s)...
12/14/22 10:38:19 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:19 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:19 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:19 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '8 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '8 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=8 -a NAME' '=' '8 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:19 From submit: Submitting job(s).
12/14/22 10:38:19 From submit: 1 job(s) submitted to cluster 8202664.
12/14/22 10:38:19 From submit: WARNING: the line 'JOB = 8' was unused by condor_submit. Is it a typo?
12/14/22 10:38:19 	assigned HTCondor ID (8202664.0.0)
12/14/22 10:38:19 Submitting HTCondor Node 9 job(s)...
12/14/22 10:38:19 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:19 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:19 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:19 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '9 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '9 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=9 -a NAME' '=' '9 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:20 From submit: Submitting job(s).
12/14/22 10:38:20 From submit: 1 job(s) submitted to cluster 8202665.
12/14/22 10:38:20 From submit: WARNING: the line 'JOB = 9' was unused by condor_submit. Is it a typo?
12/14/22 10:38:20 	assigned HTCondor ID (8202665.0.0)
12/14/22 10:38:20 Submitting HTCondor Node 10 job(s)...
12/14/22 10:38:20 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:20 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:20 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:20 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '10 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '10 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=10 -a NAME' '=' '10 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:20 From submit: Submitting job(s).
12/14/22 10:38:20 From submit: 1 job(s) submitted to cluster 8202666.
12/14/22 10:38:20 From submit: WARNING: the line 'JOB = 10' was unused by condor_submit. Is it a typo?
12/14/22 10:38:20 	assigned HTCondor ID (8202666.0.0)
12/14/22 10:38:20 Submitting HTCondor Node 11 job(s)...
12/14/22 10:38:20 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:20 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:20 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:20 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '11 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '11 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=11 -a NAME' '=' '11 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:20 From submit: Submitting job(s).
12/14/22 10:38:20 From submit: 1 job(s) submitted to cluster 8202667.
12/14/22 10:38:20 From submit: WARNING: the line 'JOB = 11' was unused by condor_submit. Is it a typo?
12/14/22 10:38:20 	assigned HTCondor ID (8202667.0.0)
12/14/22 10:38:20 Submitting HTCondor Node 12 job(s)...
12/14/22 10:38:20 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:20 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:20 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:20 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '12 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '12 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=12 -a NAME' '=' '12 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:20 From submit: Submitting job(s).
12/14/22 10:38:20 From submit: 1 job(s) submitted to cluster 8202668.
12/14/22 10:38:20 From submit: WARNING: the line 'JOB = 12' was unused by condor_submit. Is it a typo?
12/14/22 10:38:20 	assigned HTCondor ID (8202668.0.0)
12/14/22 10:38:20 Submitting HTCondor Node 13 job(s)...
12/14/22 10:38:20 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:20 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:20 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:20 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '13 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '13 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=13 -a NAME' '=' '13 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:20 From submit: Submitting job(s).
12/14/22 10:38:20 From submit: 1 job(s) submitted to cluster 8202669.
12/14/22 10:38:20 From submit: WARNING: the line 'JOB = 13' was unused by condor_submit. Is it a typo?
12/14/22 10:38:20 	assigned HTCondor ID (8202669.0.0)
12/14/22 10:38:20 Submitting HTCondor Node 14 job(s)...
12/14/22 10:38:20 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:20 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:20 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:20 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '14 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '14 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=14 -a NAME' '=' '14 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:21 From submit: Submitting job(s).
12/14/22 10:38:21 From submit: 1 job(s) submitted to cluster 8202670.
12/14/22 10:38:21 From submit: WARNING: the line 'JOB = 14' was unused by condor_submit. Is it a typo?
12/14/22 10:38:21 	assigned HTCondor ID (8202670.0.0)
12/14/22 10:38:21 Submitting HTCondor Node 15 job(s)...
12/14/22 10:38:21 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:21 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:21 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:21 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '15 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '15 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=15 -a NAME' '=' '15 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:21 From submit: Submitting job(s).
12/14/22 10:38:21 From submit: 1 job(s) submitted to cluster 8202671.
12/14/22 10:38:21 From submit: WARNING: the line 'JOB = 15' was unused by condor_submit. Is it a typo?
12/14/22 10:38:21 	assigned HTCondor ID (8202671.0.0)
12/14/22 10:38:21 Submitting HTCondor Node 16 job(s)...
12/14/22 10:38:21 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:21 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:21 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:21 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '16 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '16 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=16 -a NAME' '=' '16 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:21 From submit: Submitting job(s).
12/14/22 10:38:21 From submit: 1 job(s) submitted to cluster 8202672.
12/14/22 10:38:21 From submit: WARNING: the line 'JOB = 16' was unused by condor_submit. Is it a typo?
12/14/22 10:38:21 	assigned HTCondor ID (8202672.0.0)
12/14/22 10:38:21 Submitting HTCondor Node 17 job(s)...
12/14/22 10:38:21 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:21 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:21 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:21 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '17 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '17 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=17 -a NAME' '=' '17 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:21 From submit: Submitting job(s).
12/14/22 10:38:21 From submit: 1 job(s) submitted to cluster 8202673.
12/14/22 10:38:21 From submit: WARNING: the line 'JOB = 17' was unused by condor_submit. Is it a typo?
12/14/22 10:38:21 	assigned HTCondor ID (8202673.0.0)
12/14/22 10:38:21 Submitting HTCondor Node 18 job(s)...
12/14/22 10:38:21 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:21 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:21 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:21 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '18 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '18 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=18 -a NAME' '=' '18 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:21 From submit: Submitting job(s).
12/14/22 10:38:21 From submit: 1 job(s) submitted to cluster 8202674.
12/14/22 10:38:21 From submit: WARNING: the line 'JOB = 18' was unused by condor_submit. Is it a typo?
12/14/22 10:38:21 	assigned HTCondor ID (8202674.0.0)
12/14/22 10:38:21 Submitting HTCondor Node 19 job(s)...
12/14/22 10:38:21 Adding a DAGMan workflow log /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log
12/14/22 10:38:21 Masking the events recorded in the DAGMAN workflow log
12/14/22 10:38:21 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36
12/14/22 10:38:21 submitting: /usr/bin/condor_submit -a dag_node_name' '=' '19 -a +DAGManJobId' '=' '8202655 -a DAGManJobId' '=' '8202655 -batch-name filler.dag+8202655 -batch-id 8202655.0 -a submit_event_notes' '=' 'DAG' 'Node:' '19 -a dagman_log' '=' '/afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36" -a JOB=19 -a NAME' '=' '19 -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +KeepClaimIdle' '=' '20 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:38:21 From submit: Submitting job(s).
12/14/22 10:38:21 From submit: 1 job(s) submitted to cluster 8202675.
12/14/22 10:38:21 From submit: WARNING: the line 'JOB = 19' was unused by condor_submit. Is it a typo?
12/14/22 10:38:21 	assigned HTCondor ID (8202675.0.0)
12/14/22 10:38:21 Just submitted 20 jobs this cycle...
12/14/22 10:38:21 DAG status: 0 (DAG_STATUS_OK)
12/14/22 10:38:21 Of 21 nodes total:
12/14/22 10:38:21  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
12/14/22 10:38:21   ===     ===      ===     ===     ===        ===      ===
12/14/22 10:38:21     0       0       20       0       0          1        0
12/14/22 10:38:21 0 job proc(s) currently held
12/14/22 10:38:21 DAGMan Runtime Statistics: [ EventCycleTimeCount = 0.0; EventCycleTimeSum = 0.0; LogProcessCycleTimeCount = 0.0; LogProcessCycleTimeSum = 0.0; SleepCycleTimeCount = 0.0; SleepCycleTimeSum = 0.0; SubmitCycleTimeAvg = 3.733453035354614; SubmitCycleTimeCount = 1.0; SubmitCycleTimeMax = 3.733453035354614; SubmitCycleTimeMin = 3.733453035354614; SubmitCycleTimeStd = 3.733453035354614; SubmitCycleTimeSum = 3.733453035354614; ]
12/14/22 10:38:26 Currently monitoring 1 HTCondor log file(s)
12/14/22 10:38:26 Reassigning the id of job 0 from (8202656.0.0) to (8202656.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 0 (8202656.0.0) {12/14/22 10:38:18}
12/14/22 10:38:26 Number of idle job procs: 1
12/14/22 10:38:26 Reassigning the id of job 1 from (8202657.0.0) to (8202657.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 1 (8202657.0.0) {12/14/22 10:38:18}
12/14/22 10:38:26 Number of idle job procs: 2
12/14/22 10:38:26 Reassigning the id of job 2 from (8202658.0.0) to (8202658.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 2 (8202658.0.0) {12/14/22 10:38:18}
12/14/22 10:38:26 Number of idle job procs: 3
12/14/22 10:38:26 Reassigning the id of job 3 from (8202659.0.0) to (8202659.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 3 (8202659.0.0) {12/14/22 10:38:18}
12/14/22 10:38:26 Number of idle job procs: 4
12/14/22 10:38:26 Reassigning the id of job 4 from (8202660.0.0) to (8202660.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 4 (8202660.0.0) {12/14/22 10:38:19}
12/14/22 10:38:26 Number of idle job procs: 5
12/14/22 10:38:26 Reassigning the id of job 5 from (8202661.0.0) to (8202661.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 5 (8202661.0.0) {12/14/22 10:38:19}
12/14/22 10:38:26 Number of idle job procs: 6
12/14/22 10:38:26 Reassigning the id of job 6 from (8202662.0.0) to (8202662.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 6 (8202662.0.0) {12/14/22 10:38:19}
12/14/22 10:38:26 Number of idle job procs: 7
12/14/22 10:38:26 Reassigning the id of job 7 from (8202663.0.0) to (8202663.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 7 (8202663.0.0) {12/14/22 10:38:19}
12/14/22 10:38:26 Number of idle job procs: 8
12/14/22 10:38:26 Reassigning the id of job 8 from (8202664.0.0) to (8202664.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 8 (8202664.0.0) {12/14/22 10:38:19}
12/14/22 10:38:26 Number of idle job procs: 9
12/14/22 10:38:26 Reassigning the id of job 9 from (8202665.0.0) to (8202665.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 9 (8202665.0.0) {12/14/22 10:38:20}
12/14/22 10:38:26 Number of idle job procs: 10
12/14/22 10:38:26 Reassigning the id of job 10 from (8202666.0.0) to (8202666.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 10 (8202666.0.0) {12/14/22 10:38:20}
12/14/22 10:38:26 Number of idle job procs: 11
12/14/22 10:38:26 Reassigning the id of job 11 from (8202667.0.0) to (8202667.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 11 (8202667.0.0) {12/14/22 10:38:20}
12/14/22 10:38:26 Number of idle job procs: 12
12/14/22 10:38:26 Reassigning the id of job 12 from (8202668.0.0) to (8202668.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 12 (8202668.0.0) {12/14/22 10:38:20}
12/14/22 10:38:26 Number of idle job procs: 13
12/14/22 10:38:26 Reassigning the id of job 13 from (8202669.0.0) to (8202669.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 13 (8202669.0.0) {12/14/22 10:38:20}
12/14/22 10:38:26 Number of idle job procs: 14
12/14/22 10:38:26 Reassigning the id of job 14 from (8202670.0.0) to (8202670.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 14 (8202670.0.0) {12/14/22 10:38:21}
12/14/22 10:38:26 Number of idle job procs: 15
12/14/22 10:38:26 Reassigning the id of job 15 from (8202671.0.0) to (8202671.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 15 (8202671.0.0) {12/14/22 10:38:21}
12/14/22 10:38:26 Number of idle job procs: 16
12/14/22 10:38:26 Reassigning the id of job 16 from (8202672.0.0) to (8202672.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 16 (8202672.0.0) {12/14/22 10:38:21}
12/14/22 10:38:26 Number of idle job procs: 17
12/14/22 10:38:26 Reassigning the id of job 17 from (8202673.0.0) to (8202673.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 17 (8202673.0.0) {12/14/22 10:38:21}
12/14/22 10:38:26 Number of idle job procs: 18
12/14/22 10:38:26 Reassigning the id of job 18 from (8202674.0.0) to (8202674.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 18 (8202674.0.0) {12/14/22 10:38:21}
12/14/22 10:38:26 Number of idle job procs: 19
12/14/22 10:38:26 Reassigning the id of job 19 from (8202675.0.0) to (8202675.0.0)
12/14/22 10:38:26 Event: ULOG_SUBMIT for HTCondor Node 19 (8202675.0.0) {12/14/22 10:38:21}
12/14/22 10:38:26 Number of idle job procs: 20
12/14/22 10:41:52 Currently monitoring 1 HTCondor log file(s)
12/14/22 10:41:52 Event: ULOG_EXECUTE for HTCondor Node 0 (8202656.0.0) {12/14/22 10:41:50}
12/14/22 10:41:52 Number of idle job procs: 19
12/14/22 10:41:52 Event: ULOG_EXECUTE for HTCondor Node 2 (8202658.0.0) {12/14/22 10:41:50}
12/14/22 10:41:52 Number of idle job procs: 18
12/14/22 10:41:52 Event: ULOG_EXECUTE for HTCondor Node 1 (8202657.0.0) {12/14/22 10:41:50}
12/14/22 10:41:52 Number of idle job procs: 17
12/14/22 10:41:52 Event: ULOG_EXECUTE for HTCondor Node 3 (8202659.0.0) {12/14/22 10:41:51}
12/14/22 10:41:52 Number of idle job procs: 16
12/14/22 10:41:52 Event: ULOG_EXECUTE for HTCondor Node 9 (8202665.0.0) {12/14/22 10:41:51}
12/14/22 10:41:52 Number of idle job procs: 15
12/14/22 10:41:52 Event: ULOG_EXECUTE for HTCondor Node 4 (8202660.0.0) {12/14/22 10:41:51}
12/14/22 10:41:52 Number of idle job procs: 14
12/14/22 10:41:52 Event: ULOG_EXECUTE for HTCondor Node 8 (8202664.0.0) {12/14/22 10:41:51}
12/14/22 10:41:52 Number of idle job procs: 13
12/14/22 10:41:57 Currently monitoring 1 HTCondor log file(s)
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 10 (8202666.0.0) {12/14/22 10:41:52}
12/14/22 10:41:57 Number of idle job procs: 12
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 5 (8202661.0.0) {12/14/22 10:41:52}
12/14/22 10:41:57 Number of idle job procs: 11
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 6 (8202662.0.0) {12/14/22 10:41:52}
12/14/22 10:41:57 Number of idle job procs: 10
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 16 (8202672.0.0) {12/14/22 10:41:53}
12/14/22 10:41:57 Number of idle job procs: 9
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 12 (8202668.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 8
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 13 (8202669.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 7
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 15 (8202671.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 6
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 1 (8202657.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 6
12/14/22 10:41:57 Node 1 job proc (8202657.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 8 (8202664.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 6
12/14/22 10:41:57 Node 8 job proc (8202664.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 3 (8202659.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 6
12/14/22 10:41:57 Node 3 job proc (8202659.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 9 (8202665.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 6
12/14/22 10:41:57 Node 9 job proc (8202665.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 0 (8202656.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 6
12/14/22 10:41:57 Node 0 job proc (8202656.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 17 (8202673.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 5
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 10 (8202666.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 5
12/14/22 10:41:57 Node 10 job proc (8202666.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 2 (8202658.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 5
12/14/22 10:41:57 Node 2 job proc (8202658.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 16 (8202672.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 5
12/14/22 10:41:57 Node 16 job proc (8202672.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 5 (8202661.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 5
12/14/22 10:41:57 Node 5 job proc (8202661.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 4 (8202660.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 5
12/14/22 10:41:57 Node 4 job proc (8202660.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 6 (8202662.0.0) {12/14/22 10:41:54}
12/14/22 10:41:57 Number of idle job procs: 5
12/14/22 10:41:57 Node 6 job proc (8202662.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 18 (8202674.0.0) {12/14/22 10:41:55}
12/14/22 10:41:57 Number of idle job procs: 4
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 7 (8202663.0.0) {12/14/22 10:41:55}
12/14/22 10:41:57 Number of idle job procs: 3
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 11 (8202667.0.0) {12/14/22 10:41:55}
12/14/22 10:41:57 Number of idle job procs: 2
12/14/22 10:41:57 Event: ULOG_EXECUTE for HTCondor Node 14 (8202670.0.0) {12/14/22 10:41:56}
12/14/22 10:41:57 Number of idle job procs: 1
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 17 (8202673.0.0) {12/14/22 10:41:56}
12/14/22 10:41:57 Number of idle job procs: 1
12/14/22 10:41:57 Node 17 job proc (8202673.0.0) failed with status 127.
12/14/22 10:41:57 Event: ULOG_JOB_TERMINATED for HTCondor Node 15 (8202671.0.0) {12/14/22 10:41:56}
12/14/22 10:41:57 Number of idle job procs: 1
12/14/22 10:41:57 Node 15 job proc (8202671.0.0) failed with status 127.
12/14/22 10:41:57 DAG status: 2 (DAG_STATUS_NODE_FAILED)
12/14/22 10:41:57 Of 21 nodes total:
12/14/22 10:41:57  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
12/14/22 10:41:57   ===     ===      ===     ===     ===        ===      ===
12/14/22 10:41:57     0       0        7       0       0          1       13
12/14/22 10:41:57 0 job proc(s) currently held
12/14/22 10:41:57 DAGMan Runtime Statistics: [ EventCycleTimeAvg = 0.08718682998834654; EventCycleTimeCount = 43.0; EventCycleTimeMax = 3.733756065368652; EventCycleTimeMin = 5.91278076171875E-05; EventCycleTimeStd = 0.5693389107876945; EventCycleTimeSum = 3.749033689498901; LogProcessCycleTimeAvg = 0.004490375518798828; LogProcessCycleTimeCount = 3.0; LogProcessCycleTimeMax = 0.009608983993530273; LogProcessCycleTimeMin = 0.001595020294189453; LogProcessCycleTimeStd = 0.004445564621532552; LogProcessCycleTimeSum = 0.01347112655639648; SleepCycleTimeAvg = 5.006265568178754; SleepCycleTimeCount = 43.0; SleepCycleTimeMax = 5.040066957473755; SleepCycleTimeMin = 5.000270843505859; SleepCycleTimeStd = 0.007033453837793471; SleepCycleTimeSum = 215.2694194316864; SubmitCycleTimeAvg = 0.08491868322545831; SubmitCycleTimeCount = 44.0; SubmitCycleTimeMax = 3.733453035354614; SubmitCycleTimeMin = 4.220008850097656E-05; SubmitCycleTimeStd = 0.562828813225163; SubmitCycleTimeSum = 3.736422061920166; ]
12/14/22 10:42:02 Currently monitoring 1 HTCondor log file(s)
12/14/22 10:42:02 Event: ULOG_JOB_TERMINATED for HTCondor Node 11 (8202667.0.0) {12/14/22 10:41:57}
12/14/22 10:42:02 Number of idle job procs: 1
12/14/22 10:42:02 Node 11 job proc (8202667.0.0) failed with status 127.
12/14/22 10:42:02 Event: ULOG_JOB_TERMINATED for HTCondor Node 18 (8202674.0.0) {12/14/22 10:41:58}
12/14/22 10:42:02 Number of idle job procs: 1
12/14/22 10:42:02 Node 18 job proc (8202674.0.0) failed with status 127.
12/14/22 10:42:02 Event: ULOG_JOB_TERMINATED for HTCondor Node 7 (8202663.0.0) {12/14/22 10:41:58}
12/14/22 10:42:02 Number of idle job procs: 1
12/14/22 10:42:02 Node 7 job proc (8202663.0.0) failed with status 127.
12/14/22 10:42:02 Event: ULOG_JOB_TERMINATED for HTCondor Node 14 (8202670.0.0) {12/14/22 10:41:59}
12/14/22 10:42:02 Number of idle job procs: 1
12/14/22 10:42:02 Node 14 job proc (8202670.0.0) failed with status 127.
12/14/22 10:42:02 Event: ULOG_EXECUTE for HTCondor Node 19 (8202675.0.0) {12/14/22 10:42:00}
12/14/22 10:42:02 Number of idle job procs: 0
12/14/22 10:42:02 Event: ULOG_JOB_TERMINATED for HTCondor Node 19 (8202675.0.0) {12/14/22 10:42:01}
12/14/22 10:42:02 Number of idle job procs: 0
12/14/22 10:42:02 Node 19 job proc (8202675.0.0) failed with status 127.
12/14/22 10:42:02 DAG status: 2 (DAG_STATUS_NODE_FAILED)
12/14/22 10:42:02 Of 21 nodes total:
12/14/22 10:42:02  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
12/14/22 10:42:02   ===     ===      ===     ===     ===        ===      ===
12/14/22 10:42:02     0       0        2       0       0          1       18
12/14/22 10:42:02 0 job proc(s) currently held
12/14/22 10:42:02 DAGMan Runtime Statistics: [ EventCycleTimeAvg = 0.08526571772315285; EventCycleTimeCount = 44.0; EventCycleTimeMax = 3.733756065368652; EventCycleTimeMin = 5.91278076171875E-05; EventCycleTimeStd = 0.5628240287636133; EventCycleTimeSum = 3.751691579818726; LogProcessCycleTimeAvg = 0.02477878332138062; LogProcessCycleTimeCount = 4.0; LogProcessCycleTimeMax = 0.08564400672912598; LogProcessCycleTimeMin = 0.001595020294189453; LogProcessCycleTimeStd = 0.04073884298624914; LogProcessCycleTimeSum = 0.09911513328552246; SleepCycleTimeAvg = 5.006140307946638; SleepCycleTimeCount = 44.0; SleepCycleTimeMax = 5.040066957473755; SleepCycleTimeMin = 5.000270843505859; SleepCycleTimeStd = 0.007000670365247228; SleepCycleTimeSum = 220.2701735496521; SubmitCycleTimeAvg = 0.08303295771280925; SubmitCycleTimeCount = 45.0; SubmitCycleTimeMax = 3.733453035354614; SubmitCycleTimeMin = 4.220008850097656E-05; SubmitCycleTimeStd = 0.5565400528968545; SubmitCycleTimeSum = 3.736483097076416; ]
12/14/22 10:42:07 Currently monitoring 1 HTCondor log file(s)
12/14/22 10:42:07 Event: ULOG_JOB_TERMINATED for HTCondor Node 13 (8202669.0.0) {12/14/22 10:42:06}
12/14/22 10:42:07 Number of idle job procs: 0
12/14/22 10:42:07 Node 13 job proc (8202669.0.0) failed with status 127.
12/14/22 10:42:07 DAG status: 2 (DAG_STATUS_NODE_FAILED)
12/14/22 10:42:07 Of 21 nodes total:
12/14/22 10:42:07  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
12/14/22 10:42:07   ===     ===      ===     ===     ===        ===      ===
12/14/22 10:42:07     0       0        1       0       0          1       19
12/14/22 10:42:07 0 job proc(s) currently held
12/14/22 10:42:07 DAGMan Runtime Statistics: [ EventCycleTimeAvg = 0.08528069920010037; EventCycleTimeCount = 45.0; EventCycleTimeMax = 3.733756065368652; EventCycleTimeMin = 5.91278076171875E-05; EventCycleTimeStd = 0.5563915519710612; EventCycleTimeSum = 3.837631464004517; LogProcessCycleTimeAvg = 0.06038680076599121; LogProcessCycleTimeCount = 5.0; LogProcessCycleTimeMax = 0.2028188705444336; LogProcessCycleTimeMin = 0.001595020294189453; LogProcessCycleTimeStd = 0.08708842935529472; LogProcessCycleTimeSum = 0.3019340038299561; SleepCycleTimeAvg = 5.006012148327297; SleepCycleTimeCount = 45.0; SleepCycleTimeMax = 5.040066957473755; SleepCycleTimeMin = 5.000270843505859; SleepCycleTimeStd = 0.006973855164439874; SleepCycleTimeSum = 225.2705466747284; SubmitCycleTimeAvg = 0.08122885227203369; SubmitCycleTimeCount = 46.0; SubmitCycleTimeMax = 3.733453035354614; SubmitCycleTimeMin = 4.220008850097656E-05; SubmitCycleTimeStd = 0.5504575464442659; SubmitCycleTimeSum = 3.73652720451355; ]
12/14/22 10:42:12 Currently monitoring 1 HTCondor log file(s)
12/14/22 10:42:12 Event: ULOG_JOB_TERMINATED for HTCondor Node 12 (8202668.0.0) {12/14/22 10:42:10}
12/14/22 10:42:12 Number of idle job procs: 0
12/14/22 10:42:12 Node 12 job proc (8202668.0.0) failed with status 127.
12/14/22 10:42:12 DAG status: 2 (DAG_STATUS_NODE_FAILED)
12/14/22 10:42:12 Of 21 nodes total:
12/14/22 10:42:12  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
12/14/22 10:42:12   ===     ===      ===     ===     ===        ===      ===
12/14/22 10:42:12     0       0        0       0       0          1       20
12/14/22 10:42:12 0 job proc(s) currently held
12/14/22 10:42:12 DAGMan Runtime Statistics: [ EventCycleTimeAvg = 0.08784607182378354; EventCycleTimeCount = 46.0; EventCycleTimeMax = 3.733756065368652; EventCycleTimeMin = 5.91278076171875E-05; EventCycleTimeStd = 0.5504497464917064; EventCycleTimeSum = 4.040919303894043; LogProcessCycleTimeAvg = 0.05038801829020182; LogProcessCycleTimeCount = 6.0; LogProcessCycleTimeMax = 0.2028188705444336; LogProcessCycleTimeMin = 0.0003941059112548828; LogProcessCycleTimeStd = 0.08165396210868549; LogProcessCycleTimeSum = 0.3023281097412109; SleepCycleTimeAvg = 5.005989453066951; SleepCycleTimeCount = 46.0; SleepCycleTimeMax = 5.040066957473755; SleepCycleTimeMin = 5.000270843505859; SleepCycleTimeStd = 0.006897650263891548; SleepCycleTimeSum = 230.2755148410797; SubmitCycleTimeAvg = 0.07950165931214677; SubmitCycleTimeCount = 47.0; SubmitCycleTimeMax = 3.733453035354614; SubmitCycleTimeMin = 4.220008850097656E-05; SubmitCycleTimeStd = 0.5445701862857687; SubmitCycleTimeSum = 3.736577987670898; ]
12/14/22 10:42:12 ERROR: the following job(s) failed:
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 0
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 0
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202656.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202656.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 1
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 1
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202657.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202657.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 2
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 2
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202658.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202658.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 3
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 3
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202659.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202659.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 4
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 4
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202660.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202660.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 5
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 5
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202661.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202661.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 6
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 6
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202662.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202662.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 7
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 7
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202663.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202663.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 8
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 8
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202664.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202664.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 9
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 9
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202665.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202665.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 10
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 10
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202666.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202666.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 11
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 11
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202667.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202667.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 12
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 12
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202668.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202668.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 13
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 13
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202669.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202669.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 14
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 14
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202670.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202670.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 15
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 15
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202671.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202671.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 16
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 16
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202672.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202672.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 17
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 17
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202673.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202673.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 18
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 18
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202674.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202674.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------- Job ----------------------
12/14/22 10:42:12       Node Name: 19
12/14/22 10:42:12            Noop: false
12/14/22 10:42:12          NodeID: 19
12/14/22 10:42:12     Node Status: STATUS_ERROR    
12/14/22 10:42:12 Node return val: 127
12/14/22 10:42:12           Error: Job proc (8202675.0.0) failed with status 127
12/14/22 10:42:12 Job Submit File: /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.submit
12/14/22 10:42:12  HTCondor Job ID: (8202675.0.0)
12/14/22 10:42:12 PARENTS:  WAITING: 0 CHILDREN: Cleanup
12/14/22 10:42:12 ---------------------------------------	<END>
12/14/22 10:42:12 Aborting DAG...
12/14/22 10:42:12 Writing Rescue DAG to /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.rescue001...
12/14/22 10:42:12 Removing submitted jobs...
12/14/22 10:42:12 Removing any/all submitted HTCondor jobs...
12/14/22 10:42:12 Running: /usr/bin/condor_rm -const DAGManJobId==8202655
12/14/22 10:42:13 Note: 0 total job deferrals because of -MaxJobs limit (0)
12/14/22 10:42:13 Note: 0 total job deferrals because of -MaxIdle limit (1000)
12/14/22 10:42:13 Note: 0 total job deferrals because of node category throttles
12/14/22 10:42:13 Note: 0 total PRE script deferrals because of -MaxPre limit (20) or DEFER
12/14/22 10:42:13 Note: 0 total POST script deferrals because of -MaxPost limit (20) or DEFER
12/14/22 10:42:13 Note: 0 total HOLD script deferrals because of -MaxHold limit (20) or DEFER
12/14/22 10:42:13 DAG status: 2 (DAG_STATUS_NODE_FAILED)
12/14/22 10:42:13 Of 21 nodes total:
12/14/22 10:42:13  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
12/14/22 10:42:13   ===     ===      ===     ===     ===        ===      ===
12/14/22 10:42:13     0       0        0       0       0          1       20
12/14/22 10:42:13 0 job proc(s) currently held
12/14/22 10:42:13 DAGMan Runtime Statistics: [ EventCycleTimeAvg = 0.08784607182378354; EventCycleTimeCount = 46.0; EventCycleTimeMax = 3.733756065368652; EventCycleTimeMin = 5.91278076171875E-05; EventCycleTimeStd = 0.5504497464917064; EventCycleTimeSum = 4.040919303894043; LogProcessCycleTimeAvg = 0.05038801829020182; LogProcessCycleTimeCount = 6.0; LogProcessCycleTimeMax = 0.2028188705444336; LogProcessCycleTimeMin = 0.0003941059112548828; LogProcessCycleTimeStd = 0.08165396210868549; LogProcessCycleTimeSum = 0.3023281097412109; SleepCycleTimeAvg = 5.005989453066951; SleepCycleTimeCount = 46.0; SleepCycleTimeMax = 5.040066957473755; SleepCycleTimeMin = 5.000270843505859; SleepCycleTimeStd = 0.006897650263891548; SleepCycleTimeSum = 230.2755148410797; SubmitCycleTimeAvg = 0.07950165931214677; SubmitCycleTimeCount = 47.0; SubmitCycleTimeMax = 3.733453035354614; SubmitCycleTimeMin = 4.220008850097656E-05; SubmitCycleTimeStd = 0.5445701862857687; SubmitCycleTimeSum = 3.736577987670898; ]
12/14/22 10:42:13 Wrote metrics file /afs/cern.ch/user/g/gylee/bb_tautau/CMSSW_11_1_9/src/KLUBAnalysis/221214/filler.dag.metrics.
12/14/22 10:42:13 DAGMan Runtime Statistics: [ EventCycleTimeAvg = 0.08784607182378354; EventCycleTimeCount = 46.0; EventCycleTimeMax = 3.733756065368652; EventCycleTimeMin = 5.91278076171875E-05; EventCycleTimeStd = 0.5504497464917064; EventCycleTimeSum = 4.040919303894043; LogProcessCycleTimeAvg = 0.05038801829020182; LogProcessCycleTimeCount = 6.0; LogProcessCycleTimeMax = 0.2028188705444336; LogProcessCycleTimeMin = 0.0003941059112548828; LogProcessCycleTimeStd = 0.08165396210868549; LogProcessCycleTimeSum = 0.3023281097412109; SleepCycleTimeAvg = 5.005989453066951; SleepCycleTimeCount = 46.0; SleepCycleTimeMax = 5.040066957473755; SleepCycleTimeMin = 5.000270843505859; SleepCycleTimeStd = 0.006897650263891548; SleepCycleTimeSum = 230.2755148410797; SubmitCycleTimeAvg = 0.07950165931214677; SubmitCycleTimeCount = 47.0; SubmitCycleTimeMax = 3.733453035354614; SubmitCycleTimeMin = 4.220008850097656E-05; SubmitCycleTimeStd = 0.5445701862857687; SubmitCycleTimeSum = 3.736577987670898; ]
12/14/22 10:42:13 **** condor_scheduniv_exec.8202655.0 (condor_DAGMAN) pid 3614393 EXITING WITH STATUS 1
